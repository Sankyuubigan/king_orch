# app.py

import streamlit as st
from llama_cpp import Llama
import os
import time
import logging
import io

# --- –ù–ê–°–¢–†–û–ô–ö–ê –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø –í–ù–£–¢–†–ò STREAMLIT ---
# –°–æ–∑–¥–∞–µ–º "–ø–æ—Ç–æ–∫" –≤ –ø–∞–º—è—Ç–∏, –∫—É–¥–∞ –±—É–¥—É—Ç –ø–∏—Å–∞—Ç—å—Å—è –ª–æ–≥–∏
log_stream = io.StringIO()

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–≥–µ—Ä, —á—Ç–æ–±—ã –æ–Ω –ø–∏—Å–∞–ª –≤ –Ω–∞—à –ø–æ—Ç–æ–∫
# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —É—Ä–æ–≤–µ–Ω—å DEBUG, —á—Ç–æ–±—ã –ª–æ–≤–∏—Ç—å –∞–±—Å–æ–ª—é—Ç–Ω–æ –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è
logging.basicConfig(stream=log_stream, level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# --- –ì–õ–ê–í–ù–´–ï –ù–ê–°–¢–†–û–ô–ö–ò ---
MODELS_DIR = "D:/nn/models" # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º—ã–µ —Å–ª—ç—à–∏

st.set_page_config(page_title="The Orchestrator", page_icon="üé≠", layout="wide")
st.title("üé≠ The Orchestrator")

# --- –ë–õ–û–ö –î–õ–Ø –û–¢–û–ë–†–ê–ñ–ï–ù–ò–Ø –õ–û–ì–û–í ---
log_expander = st.expander("–ü–æ–∫–∞–∑–∞—Ç—å/—Å–∫—Ä—ã—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–µ –ª–æ–≥–∏")
log_placeholder = log_expander.empty()

# --- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ---

def find_gguf_models(directory):
    logging.info(f"–ü–æ–∏—Å–∫ –º–æ–¥–µ–ª–µ–π –≤ –ø–∞–ø–∫–µ: {directory}")
    if not os.path.isdir(directory):
        logging.error(f"–ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {directory}")
        return []
    files = [f for f in os.listdir(directory) if f.endswith('.gguf')]
    logging.info(f"–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã: {files}")
    return files

@st.cache_resource
def load_model(model_filename):
    model_path = os.path.join(MODELS_DIR, model_filename)
    logging.info(f"–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑: {model_path}")
    
    if not os.path.exists(model_path):
        logging.error(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
        st.error(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
        st.stop()
    
    with st.spinner(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ '{model_filename}'..."):
        llm = Llama(
            model_path=model_path,
            n_gpu_layers=0,
            n_ctx=4096,
            chat_format="disable",
            verbose=True # llama-cpp –±—É–¥–µ—Ç –ø–∏—Å–∞—Ç—å —Å–≤–æ–∏ –ø–æ–¥—Ä–æ–±–Ω—ã–µ –ª–æ–≥–∏
        )
    logging.info(f"–ú–æ–¥–µ–ª—å '{model_filename}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
    return llm

# --- –ò–ù–¢–ï–†–§–ï–ô–° –ò –õ–û–ì–ò–ö–ê ---

try:
    model_files = find_gguf_models(MODELS_DIR)

    if not model_files:
        st.error(f"–ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø–∞–ø–∫–µ: {MODELS_DIR}")
        st.stop()

    with st.sidebar:
        st.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏")
        selected_model_file = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å:", model_files)
        if st.button("–ù–∞—á–∞—Ç—å –Ω–æ–≤—ã–π –¥–∏–∞–ª–æ–≥"):
            if "messages" in st.session_state:
                st.session_state.messages = []
            st.rerun()

    if selected_model_file:
        llm = load_model(selected_model_file)
    else:
        st.info("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –≤ –ø–∞–Ω–µ–ª–∏ —Å–ª–µ–≤–∞.")
        st.stop()

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("–°–ø—Ä–æ—Å–∏—Ç–µ —á—Ç–æ-–Ω–∏–±—É–¥—å..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("–ú–æ–¥–µ–ª—å –¥—É–º–∞–µ—Ç..."):
                full_prompt = "<|im_start|>system\nYou are a helpful AI assistant.<|im_end|>\n"
                for msg in st.session_state.messages:
                    full_prompt += f"<|im_start|>{msg['role']}\n{msg['content']}<|im_end|>\n"
                full_prompt += "<|im_start|>assistant\n"
                
                logging.info("–ù–∞—á–∏–Ω–∞—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ—Ç–≤–µ—Ç–∞...")
                output = llm(prompt=full_prompt, max_tokens=2048, stop=["<|im_end|>"])
                result = output['choices'][0]['text'].strip()
                logging.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
                
                st.markdown(result)
                st.session_state.messages.append({"role": "assistant", "content": result})

except Exception as e:
    # –õ–æ–≤–∏–º –∞–±—Å–æ–ª—é—Ç–Ω–æ –ª—é–±—É—é –æ—à–∏–±–∫—É
    st.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
    # –ü–∏—à–µ–º –ø–æ–ª–Ω—ã–π traceback –≤ –Ω–∞—à –ª–æ–≥–≥–µ—Ä
    logging.error("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –≥–ª–∞–≤–Ω–æ–º –±–ª–æ–∫–µ:", exc_info=True)

finally:
    # –í —Å–∞–º–æ–º –∫–æ–Ω—Ü–µ, –æ–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –±–ª–æ–∫ —Å –ª–æ–≥–∞–º–∏
    log_placeholder.code(log_stream.getvalue())