import os
import signal
import subprocess
import threading
import time
import gradio as gr
import webview
import json
from ollama import Client as OllamaChatClient
from ollama import list as ollama_list
from stagehand import Stagehand

# =========================================================================
# [!!! –ù–ê–°–¢–†–û–ô–ö–ò !!!]
OLLAMA_PATH = r"D:\Projects\universal_orchestrator\ollama_runtime\ollama.exe"
DEFAULT_MODEL = "llama3:8b"
PROMPT_FILE = 'system_prompt.md'
GRADIO_SERVER_PORT = 7860
# =========================================================================

# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ ---
SYSTEM_PROMPT = "You are a helpful assistant."
agent = None
ollama_chat_client = None
window = None
ollama_process = None
app_settings = {"model": DEFAULT_MODEL, "headless": True}

# --- –§—É–Ω–∫—Ü–∏–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º ---

def load_system_prompt():
    global SYSTEM_PROMPT
    try:
        with open(PROMPT_FILE, 'r', encoding='utf-8') as f:
            SYSTEM_PROMPT = f.read()
        print(f"System prompt loaded from {PROMPT_FILE}")
    except FileNotFoundError:
        print(f"ERROR: System prompt file not found at '{PROMPT_FILE}'!")

def get_ollama_models():
    """–ü—Ä–æ—Å—Ç–∞—è, –Ω–æ –≤—ã–∑—ã–≤–∞–µ–º–∞—è –≤ –Ω—É–∂–Ω—ã–π –º–æ–º–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏—è."""
    try:
        models = ollama_list()['models']
        return [model['name'] for model in models]
    except Exception as e:
        print(f"Error fetching models: {e}")
        return []

def run_ollama_server():
    global ollama_process
    print("Starting Ollama Server...")
    ollama_process = subprocess.Popen([OLLAMA_PATH, "serve"], creationflags=subprocess.CREATE_NO_WINDOW)
    print(f"Ollama Server started with PID: {ollama_process.pid}")

def update_clients(model_name, show_browser):
    global agent, ollama_chat_client, app_settings
    print(f"Updating clients: Model -> {model_name}, Show Browser -> {show_browser}")
    app_settings["model"] = model_name
    app_settings["headless"] = not show_browser # show_browser=True -> headless=False
    try:
        agent = Stagehand(model=f"ollama/{model_name}", browser_options={"headless": app_settings["headless"]})
        ollama_chat_client = OllamaChatClient(host='http://127.0.0.1:11434')
        return f"‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã! –ê–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å: `{model_name}`."
    except Exception as e:
        print(f"Error updating clients: {e}")
        return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤: {e}"

def handle_model_creation(file_obj, model_name):
    if not file_obj or not model_name:
        yield "‚ùå –û—à–∏–±–∫–∞: –í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª –∏ —É–∫–∞–∂–∏—Ç–µ –∏–º—è.", gr.Dropdown(), gr.Button(interactive=True)
        return

    yield "‚è≥ –ò–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –º–Ω–æ–≥–æ –º–∏–Ω—É—Ç.", gr.Dropdown(), gr.Button(interactive=False)
    
    model_name = model_name.strip().lower().replace(" ", "-")
    gguf_path = file_obj.name
    modelfile_path = f"./{model_name}.modelfile"
    
    try:
        with open(modelfile_path, 'w', encoding='utf-8') as f:
            f.write(f'FROM "{os.path.abspath(gguf_path)}"')
        
        command = [OLLAMA_PATH, "create", model_name, "-f", modelfile_path]
        print(f"Running long process: {' '.join(command)}")
        
        result = subprocess.run(command, capture_output=True, text=True, check=True, encoding='utf-8')
        print(result.stdout)
        os.remove(modelfile_path)
        
        print("Model created. Waiting 5 seconds for Ollama server to settle...")
        time.sleep(5)
        
        updated_models = get_ollama_models()
        yield f"‚úÖ –ú–æ–¥–µ–ª—å '{model_name}' —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–∞!", gr.Dropdown(choices=updated_models, value=model_name), gr.Button(interactive=True)

    except Exception as e:
        yield f"‚ùå –û—à–∏–±–∫–∞: {e}", gr.Dropdown(), gr.Button(interactive=True)

# --- –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ —á–∞—Ç–∞ ---
def chat_function(message, history):
    history = history or []
    history.append([message, None])
    
    # –°–æ–±–∏—Ä–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∫ –º–æ–¥–µ–ª–∏
    conversation_for_decision = [{'role': 'system', 'content': SYSTEM_PROMPT}]
    for user_msg, assistant_msg in history[:-1]: # –í—Å–µ, –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø—É—Å—Ç–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        conversation_for_decision.append({'role': 'user', 'content': user_msg})
        if assistant_msg:
            clean_assistant_msg = assistant_msg.replace("üîé ", "", 1)
            if not (clean_assistant_msg.startswith("ü§ñ") or clean_assistant_msg.startswith("‚úÖ")):
                conversation_for_decision.append({'role': 'assistant', 'content': clean_assistant_msg})
    conversation_for_decision.append({'role': 'user', 'content': message})

    try:
        # –®–∞–≥ 1: –ü–æ–ª—É—á–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ –æ—Ç –º–æ–¥–µ–ª–∏
        response = ollama_chat_client.chat(model=app_settings["model"], messages=conversation_for_decision)
        model_response_content = response['message']['content']
        print(f"Model initial response: {model_response_content}")
        
        # –®–∞–≥ 2: –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –∫–æ–º–∞–Ω–¥—É
        try:
            tool_call = json.loads(model_response_content)
            if isinstance(tool_call, dict) and tool_call.get("tool") == "stagehand_search":
                query = tool_call.get("query")
                history[-1][1] = "ü§ñ *–ú–æ–¥–µ–ª—å —Ä–µ—à–∏–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Stagehand...*"
                yield history
                
                # –®–∞–≥ 3: –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
                stagehand_result = agent.invoke(goal=query, url="https://www.google.com")
                history[-1][1] = "‚úÖ *–ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω. –§–æ—Ä–º—É–ª–∏—Ä—É—é –æ—Ç–≤–µ—Ç...*"
                yield history
                
                # –®–∞–≥ 4: –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
                search_answer = stagehand_result.get('answer', '–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –æ—Ç–≤–µ—Ç.')
                final_prompt = [
                    {'role': 'system', 'content': '–¢—ã - –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞.'},
                    {'role': 'user', 'content': message},
                    {'role': 'system', 'content': f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}':\n{search_answer}"}
                ]
                
                final_response_stream = ollama_chat_client.chat(model=app_settings["model"], messages=final_prompt, stream=True)
                
                full_response = ""
                for chunk in final_response_stream:
                    full_response += chunk['message']['content']
                    history[-1][1] = f"üîé {full_response}"
                    yield history
                return
        except (json.JSONDecodeError, TypeError):
            # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ JSON - —ç—Ç–æ –æ–±—ã—á–Ω—ã–π –æ—Ç–≤–µ—Ç
            history[-1][1] = model_response_content
            yield history
            
    except Exception as e:
        print(f"CRITICAL ERROR in chat_function: {e}")
        import traceback
        traceback.print_exc()
        history[-1][1] = f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}"
        yield history

# --- –°–±–æ—Ä–∫–∞ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ Gradio ---
def create_gradio_app(models):
    with gr.Blocks(theme=gr.themes.Soft(), title="The Orchestrator") as app:
        gr.Markdown("# The Orchestrator (Agent Edition)")
        
        with gr.Tabs():
            with gr.TabItem("–ß–∞—Ç"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —á–∞—Ç–∞")
                        status_display = gr.Markdown("‚úÖ –°—Ç–∞—Ç—É—Å: –ì–æ—Ç–æ–≤")
                        model_dropdown = gr.Dropdown(label="–ê–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å", choices=models, value=app_settings["model"])
                        headless_checkbox = gr.Checkbox(label="–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –±—Ä–∞—É–∑–µ—Ä Stagehand", value=False)
                        apply_button = gr.Button("–ü—Ä–∏–º–µ–Ω–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
                    with gr.Column(scale=3):
                        chatbot = gr.Chatbot(height=600, label="–ß–∞—Ç")
                        msg_textbox = gr.Textbox(placeholder="–°–ø—Ä–æ—Å–∏ –º–µ–Ω—è –æ —á–µ–º-–Ω–∏–±—É–¥—å...", container=False, scale=7)
                        send_button = gr.Button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å")

            with gr.TabItem("–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏"):
                gr.Markdown("### –î–æ–±–∞–≤–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑ GGUF —Ñ–∞–π–ª–∞")
                with gr.Row():
                    gguf_file_upload = gr.File(label="1. –í—ã–±–µ—Ä–∏—Ç–µ .gguf —Ñ–∞–π–ª", file_types=['.gguf'])
                    new_model_name = gr.Textbox(label="2. –ü—Ä–∏–¥—É–º–∞–π—Ç–µ –∫–æ—Ä–æ—Ç–∫–æ–µ –∏–º—è")
                create_model_button = gr.Button("3. –î–æ–±–∞–≤–∏—Ç—å –º–æ–¥–µ–ª—å", variant="primary")
                creation_status = gr.Markdown("")

        def submit_message(message, history):
            for h in chat_function(message, history):
                yield h, ""

        send_button.click(fn=submit_message, inputs=[msg_textbox, chatbot], outputs=[chatbot, msg_textbox])
        msg_textbox.submit(fn=submit_message, inputs=[msg_textbox, chatbot], outputs=[chatbot, msg_textbox])

        apply_button.click(fn=update_clients, inputs=[model_dropdown, headless_checkbox], outputs=[status_display])
        create_model_button.click(fn=handle_model_creation, inputs=[gguf_file_upload, new_model_name], outputs=[creation_status, model_dropdown, create_model_button])
    return app

# --- –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è ---
def start_gradio(app):
    app.launch(server_name="127.0.0.1", server_port=GRADIO_SERVER_PORT)

def on_closing():
    print("Window is closing, shutting down servers...")
    if ollama_process: ollama_process.terminate()
    print("Shutdown complete.")

def wait_for_ollama(max_retries=15):
    print("Waiting for Ollama server to be ready...")
    for i in range(max_retries):
        try:
            ollama_list()
            print("‚úÖ Ollama server is ready.")
            return True
        except Exception:
            print(f"Attempt {i+1}/{max_retries}... server not ready.")
            time.sleep(2)
    print("‚ùå Ollama server did not start in time.")
    return False

if __name__ == '__main__':
    load_system_prompt()
    run_ollama_server()
    
    if wait_for_ollama():
        # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
        initial_models = get_ollama_models()
        if not initial_models:
            print("CRITICAL: No models found. Exiting.")
            if ollama_process: ollama_process.terminate()
            exit()
        
        app_settings["model"] = initial_models[0]
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç—ã –ü–û–°–õ–ï —Ç–æ–≥–æ, –∫–∞–∫ —Å–µ—Ä–≤–µ—Ä —Ç–æ—á–Ω–æ –≥–æ—Ç–æ–≤
        update_clients(app_settings["model"], False)
        
        # –ü–µ—Ä–µ–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –≤ UI
        app = create_gradio_app(initial_models)
        
        gradio_thread = threading.Thread(target=start_gradio, args=(app,))
        gradio_thread.daemon = True
        gradio_thread.start()
        
        print(f"Gradio thread started. Waiting for UI at http://127.0.0.1:{GRADIO_SERVER_PORT}")
        time.sleep(2)

        window = webview.create_window('The Orchestrator', f'http://127.0.0.1:{GRADIO_SERVER_PORT}', width=1024, height=768)
        window.events.closing += on_closing
        webview.start()
    else:
        print("Could not start the application because Ollama server failed to respond.")
        if ollama_process: ollama_process.terminate()

    print("Application closed.")