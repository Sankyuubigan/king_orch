import os
import signal
import subprocess
import threading
import time
import gradio as gr
import webview
import json
import shutil
from ollama import Client as OllamaChatClient
from ollama import list as ollama_list

# =========================================================================
# [!!! –ù–ê–°–¢–†–û–ô–ö–ò !!!]
OLLAMA_PATH = r"D:\Projects\universal_orchestrator\ollama_runtime\ollama.exe"
DEFAULT_MODEL = "llama3:8b"
PROMPT_FILE = 'system_prompt.md'
GRADIO_SERVER_PORT = 7860
# =========================================================================

# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ ---
SYSTEM_PROMPT = "You are a helpful assistant."
agent = None
ollama_chat_client = None
window = None
ollama_process = None
app_settings = {"model": DEFAULT_MODEL, "headless": True}

# --- –§—É–Ω–∫—Ü–∏–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º ---

def load_system_prompt():
    global SYSTEM_PROMPT
    try:
        with open(PROMPT_FILE, 'r', encoding='utf-8') as f:
            SYSTEM_PROMPT = f.read()
        print(f"System prompt loaded from {PROMPT_FILE}")
    except FileNotFoundError:
        print(f"ERROR: System prompt file not found at '{PROMPT_FILE}'!")

def get_ollama_models():
    try:
        models = ollama_list()['models']
        return [model['name'] for model in models]
    except Exception:
        return [DEFAULT_MODEL]

def run_ollama_server():
    global ollama_process
    print("Starting Ollama Server...")
    ollama_process = subprocess.Popen([OLLAMA_PATH, "serve"], creationflags=subprocess.CREATE_NO_WINDOW)
    print(f"Ollama Server started with PID: {ollama_process.pid}")

def update_clients(model_name, show_browser):
    global agent, ollama_chat_client, app_settings
    print(f"Updating settings: Model -> {model_name}, Show Browser -> {show_browser}")
    app_settings["model"] = model_name
    app_settings["headless"] = not show_browser
    try:
        agent = Stagehand(model=f"ollama/{model_name}", browser_options={"headless": app_settings["headless"]})
        ollama_chat_client = OllamaChatClient(host='http://127.0.0.1:11434')
        status_message = f"‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã! –ê–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å: `{model_name}`."
        print(status_message)
        return status_message
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞: {e}"

def handle_model_creation(file_obj, model_name):
    """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –≤ Ollama –∏–∑ GGUF —Ñ–∞–π–ª–∞."""
    if not file_obj or not model_name:
        return "‚ùå –û—à–∏–±–∫–∞: –í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª –∏ —É–∫–∞–∂–∏—Ç–µ –∏–º—è –¥–ª—è –º–æ–¥–µ–ª–∏.", gr.Dropdown(choices=get_ollama_models(), value=app_settings["model"])
    
    model_name = model_name.strip().lower().replace(" ", "-")
    gguf_path = file_obj.name
    modelfile_path = f"./{model_name}.modelfile"
    
    try:
        print(f"Creating Modelfile for {model_name} at {modelfile_path}")
        with open(modelfile_path, 'w') as f:
            f.write(f'FROM "{os.path.abspath(gguf_path)}"')
        
        print(f"Running: ollama create {model_name} -f {modelfile_path}")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –∏ –∂–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        result = subprocess.run(
            ["ollama", "create", model_name, "-f", modelfile_path],
            capture_output=True, text=True, check=True
        )
        print(result.stdout)
        os.remove(modelfile_path) # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π Modelfile
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –≤ –≤—ã–ø–∞–¥–∞—é—â–µ–º –º–µ–Ω—é
        updated_models = get_ollama_models()
        return f"‚úÖ –ú–æ–¥–µ–ª—å '{model_name}' —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞!", gr.Dropdown(choices=updated_models, value=model_name)

    except subprocess.CalledProcessError as e:
        print(f"Ollama create error: {e.stderr}")
        return f"‚ùå –û—à–∏–±–∫–∞ Ollama: {e.stderr}", gr.Dropdown(choices=get_ollama_models(), value=app_settings["model"])
    except Exception as e:
        print(f"An error occurred: {e}")
        return f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞: {e}", gr.Dropdown(choices=get_ollama_models(), value=app_settings["model"])


# --- –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ —á–∞—Ç–∞ ---
def chat_function(message, history):
    print(f"User message: {message}")
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏
    conversation = [{'role': 'system', 'content': SYSTEM_PROMPT}]
    for user_msg, assistant_msg in history:
        conversation.append({'role': 'user', 'content': user_msg})
        clean_assistant_msg = assistant_msg.replace("üîé ", "", 1)
        if not clean_assistant_msg.startswith("ü§ñ") and not clean_assistant_msg.startswith("‚úÖ"):
            conversation.append({'role': 'assistant', 'content': clean_assistant_msg})
    conversation.append({'role': 'user', 'content': message})

    try:
        response = ollama_chat_client.chat(model=app_settings["model"], messages=conversation)
        model_response_content = response['message']['content']
        print(f"Model initial response: {model_response_content}")

        try:
            tool_call = json.loads(model_response_content)
            if tool_call.get("tool") == "stagehand_search":
                # ... (–ª–æ–≥–∏–∫–∞ Stagehand –æ—Å—Ç–∞–ª–∞—Å—å –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
                query = tool_call.get("query")
                yield "ü§ñ *–ú–æ–¥–µ–ª—å —Ä–µ—à–∏–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Stagehand. –í—ã–ø–æ–ª–Ω—è—é –ø–æ–∏—Å–∫...*"
                stagehand_result = agent.invoke(goal=query, url="https://www.google.com")
                search_answer = stagehand_result.get('answer', '–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –æ—Ç–≤–µ—Ç.')
                yield "‚úÖ *–ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω. –§–æ—Ä–º—É–ª–∏—Ä—É—é –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç...*"
                final_prompt = conversation + [{'role': 'assistant', 'content': model_response_content}, {'role': 'system', 'content': f"–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞: {search_answer}\n–ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –¥–∞–π –æ—Ç–≤–µ—Ç."}]
                final_response_stream = ollama_chat_client.chat(model=app_settings["model"], messages=final_prompt, stream=True)
                full_response = ""
                for chunk in final_response_stream:
                    full_response += chunk['message']['content']
                    yield f"üîé {full_response}"
                return
        except (json.JSONDecodeError, TypeError, KeyError):
            yield model_response_content
    except Exception as e:
        yield f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}"

# --- –°–±–æ—Ä–∫–∞ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ Gradio ---
with gr.Blocks(theme=gr.themes.Soft(), title="The Orchestrator") as app:
    gr.Markdown("# The Orchestrator (Agent Edition)")
    
    with gr.Tabs():
        with gr.TabItem("–ß–∞—Ç"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —á–∞—Ç–∞")
                    model_dropdown = gr.Dropdown(label="–ê–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å", choices=get_ollama_models(), value=DEFAULT_MODEL)
                    headless_checkbox = gr.Checkbox(label="–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –±—Ä–∞—É–∑–µ—Ä Stagehand", value=False)
                    apply_button = gr.Button("–ü—Ä–∏–º–µ–Ω–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
                    status_display = gr.Markdown("")
                with gr.Column(scale=3):
                    gr.ChatInterface(fn=chat_function, chatbot=gr.Chatbot(height=600, label="–ß–∞—Ç"), textbox=gr.Textbox(placeholder="–°–ø—Ä–æ—Å–∏ –º–µ–Ω—è –æ —á–µ–º-–Ω–∏–±—É–¥—å...", container=False, scale=7))
        
        with gr.TabItem("–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏"):
            gr.Markdown("### –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –∏–∑ GGUF —Ñ–∞–π–ª–∞")
            with gr.Row():
                gguf_file_upload = gr.File(label="–í—ã–±–µ—Ä–∏—Ç–µ .gguf —Ñ–∞–π–ª", file_types=['.gguf'])
                new_model_name = gr.Textbox(label="–ü—Ä–∏–¥—É–º–∞–π—Ç–µ –∫–æ—Ä–æ—Ç–∫–æ–µ –∏–º—è –¥–ª—è –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'dolphin-cool')")
            create_model_button = gr.Button("–°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å")
            creation_status = gr.Markdown("")

    # –ü—Ä–∏–≤—è–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –∫ –∫–Ω–æ–ø–∫–∞–º
    apply_button.click(fn=update_clients, inputs=[model_dropdown, headless_checkbox], outputs=[status_display])
    create_model_button.click(
        fn=handle_model_creation, 
        inputs=[gguf_file_upload, new_model_name], 
        outputs=[creation_status, model_dropdown] # –û–±–Ω–æ–≤–ª—è–µ–º –∏ —Å—Ç–∞—Ç—É—Å, –∏ —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
    )

# --- –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è ---
def start_gradio():
    app.launch(server_name="127.0.0.1", server_port=GRADIO_SERVER_PORT)

def on_closing():
    print("Window is closing, shutting down servers...")
    if ollama_process: ollama_process.terminate()
    print("Shutdown complete.")

if __name__ == '__main__':
    load_system_prompt()
    run_ollama_server()
    time.sleep(3) # –î–∞–µ–º Ollama –≤—Ä–µ–º—è –Ω–∞ –∑–∞–ø—É—Å–∫ –ø–µ—Ä–µ–¥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π
    update_clients(DEFAULT_MODEL, False)

    gradio_thread = threading.Thread(target=start_gradio)
    gradio_thread.daemon = True
    gradio_thread.start()
    
    time.sleep(3)

    window = webview.create_window('The Orchestrator', f'http://127.0.0.1:{GRADIO_SERVER_PORT}', width=1024, height=768)
    window.events.closing += on_closing
    webview.start()

    print("Application closed.")